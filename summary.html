<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="summary-of-on-the-likelihood-that-one-unknown-probability-exceeds-another-in-the-view-of-the-evidence-of-two-sample-by-william-r.-thompson">Summary of <em>On the likelihood that one unknown probability exceeds another in the view of the evidence of two sample</em> by William R. Thompson</h1>
<h2 id="section-1">Section 1</h2>
<h3 id="first-paragraph">First Paragraph</h3>
<p>Establishes the need to apply more rigorous methods to experimental data from medical research than is presently typically employed.</p>
<blockquote>
<p>not centred upon the interpretation of particular data, but ... a general interest in problems of research planning ... there can be no objection to the use of data, <em>however meagre,</em> as a guide to action required before more can be collected. [Emphasis added.]</p>
</blockquote>
<p>Here, Thompson is claiming that in the context of medical research, we are obligated to use all of the available data in order to inform our decisions about experiment design and treatment, even if it is a very small sample.</p>
<blockquote>
<p>Serious objection can otherwise be raised to argument based upon a small number of observations. Indeed, the fact that such objection can never be eliminated entirely---no matter how great the number of observations---suggested the possible value of seeking other modes of operation than that of taking a large number of observations before analysis or any attempt to direct our course.</p>
</blockquote>
<p>The danger of generalizing from insufficient evidence is obvious. Less obvious is the answer to the question: how much evidence constitutes sufficient evidence? And if 10,000 observations is sufficient evidence, then is 9999 not enough to generalize?</p>
<p>Thompson is making a case that confidence be treated as a <em>sliding scale</em>. If we can measure the amount of evidence an observation constitutes for the value of an unknown quantity, then we can measure how much confidence we can derive from that observation. Then, knowledge of <em>both,</em> (1) the estimate for the unknown, and (2) the measurement of our confidence in the estimate, can be used to guide our decisions.</p>
<p>Thompson is laying the foundations for applying decision theory to medical research.</p>
<h3 id="second-paragraph">Second Paragraph</h3>
<p>Thompson provides an example scenario. There are two potential treatments for a life-threatening condition, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. A patient treated by either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> might subsequently die. It is unknown which treatment is <em>better</em> (by some unstated formal criteria). <span class="math inline">\(p\)</span> is the probability that treatment <span class="math inline">\(A\)</span> is the better, thus, <span class="math inline">\(1 - p\)</span> is the probability that <span class="math inline">\(B\)</span> is the better. The problem: we have <span class="math inline">\(n\)</span> patients, and we must somehow decide how to treat them.</p>
<h3 id="third-paragraph">Third Paragraph</h3>
<p>Continuation of the above example: suppose <span class="math inline">\(0.5 &lt; p &lt; 1\)</span>.</p>
<p>Now, suppose we permanently eliminate treatment <span class="math inline">\(B\)</span> for all <span class="math inline">\(n\)</span> patients. At the time we eliminated <span class="math inline">\(B\)</span>, there was still a <span class="math inline">\(1 - p\)</span> chance that <span class="math inline">\(B\)</span> was the better treatment. All <span class="math inline">\(n\)</span> patients treated with treatment <span class="math inline">\(A\)</span> instead of treatment <span class="math inline">\(B\)</span> have a <span class="math inline">\(1 - p &gt; 0\)</span> chance that they are receiving the inferior treatment.</p>
<p>From a frequentest point of view, this means that for large <span class="math inline">\(n\)</span>, we have <span class="math inline">\((1 - p) n\)</span> patients receiving the inferior treatment, which might sound acceptable if <span class="math inline">\(1 - p\)</span> is small. However, the frequentest interpretation is not really appropriate here, because <em>all</em> patients receive the same treatment, namely <span class="math inline">\(A\)</span>. So, even if <span class="math inline">\(1 - p\)</span> is small, we still have a <span class="math inline">\(1 - p\)</span> chance of subjecting <em>all</em> <span class="math inline">\(n\)</span> patients to the inferior treatment. We have <span class="math inline">\(n (1 - p)\)</span> regret.</p>
<p>Next, suppose we select <span class="math inline">\(0.5 n\)</span> patients to receive treatment <span class="math inline">\(A\)</span> and <span class="math inline">\(0.5 n\)</span> patients to receive treatment <span class="math inline">\(B\)</span>. Since <span class="math inline">\(p &gt; 0.5\)</span>, the overall outcome for the <span class="math inline">\(n\)</span> patients is <em>worse</em> than if we had actually given them all treatment <span class="math inline">\(A\)</span>. This outcome, however, might be considered acceptable if the chief aim of treatment is to gather more data rather than to save the lives of the patients. We have <span class="math inline">\(0.5 n (1 - p) + 0.5 n p = 0.5 n\)</span> regret.</p>
<p>Finally, suppose we choose <span class="math inline">\(p n\)</span> patients to receive treatment <span class="math inline">\(A\)</span> and <span class="math inline">\((1 - p) n\)</span> patients to receive treatment <span class="math inline">\(B\)</span>. We have <span class="math inline">\(p n (1 - p) + (1 - p) n p = 2 n p (1 - p)\)</span> regret.</p>
<p><span class="math inline">\(2 n p (1 - p)\)</span> has its maximum at <span class="math inline">\(p = 0.5\)</span>. Since <span class="math inline">\(p &gt; 0.5\)</span>, <span class="math inline">\(2 n p (1 - p)\)</span> produces less regret than the 50/50 split described above.</p>
<p>Notice, though that <span class="math inline">\(2 n p (1 - p)\)</span> is <em>not</em> less than <span class="math inline">\(n (1 - p)\)</span>. This is why Thompson emphasizes the word <em>temporarily</em> in &quot;the expectation of such sacrifice would be <em>temporarily</em> ... 2 p (1 - p)&quot;.</p>
<blockquote>
<p>In the <em>long run,</em> if a real preference exists between the two <em>treatments,</em> the expected saving by continued application of this method of apportionment rather than making immediate final decision is sensibly <span class="math inline">\(1 - p\)</span> of individuals subsequently treated.</p>
</blockquote>
<p>Thought it's not explicitly stated, Thompson anticipates updating <span class="math inline">\(p\)</span> throughout the trial process, based on the results of each treatment. As we gain more certainty that <span class="math inline">\(A\)</span> is the better, <span class="math inline">\(p\)</span> will increase, approaching <span class="math inline">\(1\)</span>. The closer <span class="math inline">\(p\)</span> gets to <span class="math inline">\(1\)</span>, the closer <span class="math inline">\(2 n p (1 - p)\)</span> gets to <span class="math inline">\(0\)</span>, whereas if we eliminate treatment <span class="math inline">\(B\)</span>, then <span class="math inline">\(p\)</span> is fixed for all time, fixing our regret at <span class="math inline">\(n (1 - p)\)</span>.</p>
<h3 id="fourth---sixth-paragraph">Fourth---Sixth Paragraph</h3>
<p>Continuing the above example, we need methods for updating <span class="math inline">\(p\)</span> based on the results of our decisions. This is a rather general problem in probability theory not limited to medical research.</p>
<p><em>Section 2</em> gives an example where we do exactly that: we make a decisions, we take note of the outcome, and we update our beliefs according to the observed outcome.</p>
<h3 id="seventh-paragraphs">Seventh Paragraphs</h3>
<p>Thompson makes the case that future medical trials should, wherever possible, be reduced to the methods demonstrated in <em>Section 2</em>. Although it is unstated, Thompson's prescription is motivated by the desire to save the most patients in the <em>long run</em>, i.e. when the number of patients is arbitrarily large.</p>
<h2 id="section-2">Section 2</h2>
<h3 id="first-paragraph-1">First Paragraph</h3>
<p>We have two infinite populations <span class="math inline">\(\Omega_i\)</span> for <span class="math inline">\(i \in \{ 1, 2 \}\)</span>. On each population, there is a random variable <span class="math inline">\(X_i\)</span>:</p>
<p><span class="math display">\[
X_i \sim \textrm{Bernoulli}(\tilde{p}_i)
\]</span></p>
<p>Number of individuals exhibiting <span class="math inline">\(X_i = 1\)</span> out of <span class="math inline">\(n\)</span> drawn from population <span class="math inline">\(\Omega_i\)</span> is a random variable <span class="math inline">\(R_i^n\)</span>:</p>
<p><span class="math display">\[
R_i^n \sim \textrm{Binomial}(n, \tilde{p}_i)
\]</span></p>
<p>Starting with a uniform prior for <span class="math inline">\(\tilde{p}_i\)</span>, after <span class="math inline">\(n_i\)</span> trials with <span class="math inline">\(r_i\)</span> <em>successes</em> (samples where <span class="math inline">\(X_i = 1\)</span>) and <span class="math inline">\(s_i\)</span> failures (samples where <span class="math inline">\(X_i = 0\)</span>), our estimate of $_i is as follows:</p>
<p><span class="math display">\[
\tilde{p}_i \sim \mathrm{Beta}(r_i + 1, s_i + 1)
\]</span></p>
<p>These facts had been worked out previously in papers by Pearson and Müller, whom Thompson cites.</p>
<p>Modern statisticians and probability theorist will recognize the Beta distribution as being the <em>conjugate prior</em> for the Bernoulli distribution, a conceptualization that summarizes the relationship stated above. Thompson has the handicap of working from a lack of conceptual foundation; for example, the terms &quot;Beta distribution&quot; and &quot;conjugate prior&quot; do not appear in Thompson's paper. In fact, Thompson's paper predates the canonization of the Beta distribution, which (when scaled and shifted) would have been called &quot;Pearson's Type I Distribution&quot; at the time of the paper. As such, there were no tables available giving values for or samples from Beta distributions for various parameters. Thus, the purpose of <em>Section 2</em> is to numerically describe <span class="math inline">\(\mathrm{Beta}(r_i + 1, s_i + 1)\)</span> for small values of <span class="math inline">\(r_i, s_i\)</span>.</p>
<h3 id="second-paragraph-1">Second Paragraph</h3>
<blockquote>
<p>Obviously, we may write [insert Wall-of-Math™ here].</p>
</blockquote>
<p>I tend to stop reading when the word &quot;obviously&quot; rears its head. Thanks, Dr. Thompson.</p>
<p>Courtesy to the reader aside, we have in this paragraph the answer to the central problem of the paper:</p>
<p><span class="math display">\[
P(\tilde{p}_2 &gt; \tilde{p}_1) = \frac{
  \sum_{\alpha = 0}^{s_1} {s_1 + s_2 - \alpha \choose s_2} {r_1 + r_2 + 1 + \alpha \choose r_2}
}{
  {n_1 + n_2 + 2 \choose n_1 + 1}
}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(r_1, r_2\)</span> are the number of observed successes,</li>
<li><span class="math inline">\(s_1, s_2\)</span> are the number of observed failures,</li>
<li><span class="math inline">\(n_1 = r_1 + s_1, n_2 = r_2 + s_2\)</span> are the number of samples,</li>
</ul>
<p>obtained by taking <span class="math inline">\(n_1, n_2\)</span> samples from each of <span class="math inline">\(\Omega_1, \Omega_2\)</span>.</p>
<h3 id="third-paragraph-1">Third Paragraph</h3>
<p>Thompson is cleverly pointing out some symmetry in the problem statement that will be exploited later to facilitate some computations. Use of the term &quot;obvious&quot; here is justified (relative to audience, or course), since he's simply reminding the reader of some basic facts of probability theory.</p>
<h3 id="fourth-paragraph">Fourth Paragraph</h3>
<p>I find it odd that Thompson has to explain how to use Pascal's Triangle, especially in the days before electronic computers.</p>
<h3 id="fifth-paragraph">Fifth Paragraph</h3>
<p>Thompson points out his use of Stirling's factorial approximation formula to facilitate computations.</p>
<h3 id="sixth-paragraph">Sixth Paragraph</h3>
<p><span class="math inline">\(\psi(r_1, s_1, r_2, s_2)\)</span> is simply defined to be the expression in <em>Equation 5</em>, which in the context of our problem is the value we are interested in computing for small values of <span class="math inline">\(r_1, s_1, r_2, s_2\)</span>:</p>
<p><span class="math display">\[
\psi(r_1, s_1, r_2, s_2) = \frac{
  \sum_{\alpha = 0}^{s_1} {s_1 + s_2 - \alpha \choose s_2} {r_1 + r_2 + 1 + \alpha \choose r_2}
}{
  {n_1 + n_2 + 2 \choose n_1 + 1}
}
\]</span></p>
<p>I am confused about what Thompson is claiming with the limit in Equation 11.</p>
<h3 id="seventh-paragraph">Seventh Paragraph</h3>
<p>Lots of things are obvious to Dr. Thompson. His style of prose strikes me as being a cop-out, not to mention decidedly immodest.</p>
<p>The seventh paragraph is a proof of the limit statement described in the sixth, though I can't say that I follow it entirely. Since it's obvious, though, we'll give it a pass for now.</p>
<p>He makes a lot of omissions in his notation. The <span class="math inline">\(\bar{R}\)</span>, <span class="math inline">\(\omega_1\)</span>, and <span class="math inline">\(\omega_2\)</span> are more appropriately understood as being functions of <span class="math inline">\(n_1\)</span>, thus rewriting Equation 15 as</p>
<p><span class="math display">\[
\omega_1(n_1) &lt; \bar{R}(n_1) &lt; \omega_2(n_1)
\text{, and }
\lim_{n_1 \to \infty} [\bar{R}(n_1)] = 1
\text{.}
\]</span></p>
<p>gives a general indication of the overall shape of Thompson's argument in this paragraph. Unfortunately, Dr. Thompson leaves the scopes of the <span class="math inline">\(\mathrm{Min}\)</span> and <span class="math inline">\(\mathrm{Max}\)</span> operators to a context from which I cannot infer them.</p>
<h3 id="eighth-paragraph">Eighth Paragraph</h3>
<p>Multiply both sides by the (constant) denominator.</p>
<h3 id="ninth-paragraph">Ninth Paragraph</h3>
<p>Thompson here draws comparisons to Pearson's work in a vernacular that I am unacquainted with. The point is, again, to make use of symmetry to facilitate computations.</p>
<h3 id="tenth-paragraph">Tenth Paragraph</h3>
<p>Finally, some results! Using the machinations built up previously, we derive:</p>
<p><span class="math display">\[
\psi(r_1, s_1, 0, 0) = \frac{
  s_1 + 1
}{
  r_1 + s_1 + 2
}
\]</span></p>
<p>In other words, on the basis of only observations from population <span class="math inline">\(\Omega_1\)</span>, we can make conclusions about the likelihood that <span class="math inline">\(\tilde{p}_2\)</span> is greater than <span class="math inline">\(\tilde{p}_1\)</span> (when we assume uniform priors for our estimates of <span class="math inline">\(\tilde{p}_1\)</span> and <span class="math inline">\(\tilde{p}_2\)</span>).</p>
<p>The cases where one parameter is negative serve simply as formal idealizations in the context of the problem at hand.</p>
<h3 id="eleventh-paragraph">Eleventh Paragraph</h3>
<p>Thompson draws comparisons to Pearson's work. Pearson was using similar methods to address a related problem, and the solutions of many of Pearson's questions are answered by the values of the <span class="math inline">\(psi\)</span> function as well, for certain choices of the parameters. Thus, the values of the <span class="math inline">\(psi\)</span> function are useful for answering a range of related questions.</p>
<h3 id="twelfth---fourteenth-paragraph">Twelfth---Fourteenth Paragraph</h3>
<p>A new notation is introduced to help facilitate computations: <span class="math inline">\(N(r, s, r&#39;, s&#39;)\)</span> and <span class="math inline">\(D(r, s, r&#39;, s&#39;)\)</span>.</p>
<p><span class="math display">\[
N(r, s, r&#39;, s&#39;) =
  \sum_{\alpha = 0}^{r&#39;}
  {r + r&#39; - \alpha \choose r}
  {s + s&#39; + 1 \alpha \choose s}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
D(r, s, r&#39;, s&#39;) = D(n, n&#39;) =
  {n + n&#39; + 2 \choose n + 1}
\]</span></p>
<p>where <span class="math inline">\(n = r + s, n&#39; = r&#39; + s&#39;\)</span>.</p>
<p>In this notation, we have <span class="math inline">\(\psi(r, s, r&#39;, s&#39;) = N(r, s, r&#39;, s&#39;) / D(r, s, r&#39;, s&#39;)\)</span>.</p>
<p>Several inductive formulas and base cases are derived for <span class="math inline">\(N\)</span> and <span class="math inline">\(D\)</span>, whereby <span class="math inline">\(N\)</span> and <span class="math inline">\(D\)</span> may be calculated recursively.</p>
<h3 id="fifteenth-paragraph">Fifteenth Paragraph</h3>
<p>The inductive relations above do not completely characterize the <span class="math inline">\(N\)</span> and <span class="math inline">\(D\)</span> functions. In order to calculate <span class="math inline">\(\phi\)</span>, we still need a table of values of <span class="math inline">\(N\)</span> and <span class="math inline">\(D\)</span> for all tuples of positive integers <span class="math inline">\((r, s, r&#39;, s&#39;)\)</span> where (1) <span class="math inline">\(r + s \geq r&#39; + s&#39;\)</span>, and (2) <span class="math inline">\(r \geq s\)</span>.</p>
<h3 id="sixteenth-paragraph">Sixteenth Paragraph</h3>
<p>Expanding the above recursive relations on the <span class="math inline">\(N\)</span> function, we find that calculating its values reduces to summing certain values of the <span class="math inline">\(D\)</span> functions, and we are given</p>
<p><span class="math display">\[
N(r, s, r&#39;, s&#39;) =
  \sum_{\alpha = 0}^{r&#39;}
  D(r - 1, r&#39; - 1 - \alpha)
  D(s - 1, s&#39; + \alpha)
\]</span></p>
<p>thus we really only need to calculate the values of the (two-parameter) <span class="math inline">\(D\)</span> function.</p>
<p>Finally, Thompson provides us a table of values of <span class="math inline">\(N\)</span> and <span class="math inline">\(D\)</span> for some small values of the arguments. Recall <span class="math inline">\(\psi\)</span> is the probability that <span class="math inline">\(\tilde{p}_2 &gt; \tilde{p}_1\)</span> given <span class="math inline">\(r\)</span> successes and <span class="math inline">\(s\)</span> failures from Population 1 and <span class="math inline">\(r&#39;\)</span> successes and <span class="math inline">\(s&#39;\)</span> failures from Population 2. Thus, the table solves the posed problem for small numbers of samples.</p>
<h2 id="section-3">Section 3</h2>
<h3 id="first-paragraph-2">First Paragraph</h3>
<p>The table and identities of <em>Section 2</em> cover all cases where <span class="math inline">\(n, n&#39; \leq 5\)</span> and make it possible in those cases to select a treatment based on the probability that it is the better treatment, as suggested in <em>Section 1</em>.</p>
<h3 id="second-paragraph-2">Second Paragraph</h3>
<p>It'd be very beneficial to have a table of values of <span class="math inline">\(D(n, n&#39;)\)</span>, or even just approximate values.</p>
<h3 id="third-paragraph-2">Third Paragraph</h3>
<p>Tabulation of the <span class="math inline">\(psi\)</span> function and Pearson's <em>hypergeometric series</em> are equivalent.</p>
<h3 id="fourth-paragraph-1">Fourth Paragraph</h3>
<p>A future paper will return to the topic of the <em>method of apportionment</em> (i.e., the actual Thompson Sampling algorithm).</p>
</body>
</html>
